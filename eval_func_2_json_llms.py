from pathlib import Path
import json 
import time
from adapt_and_manipulate_scenario.functional_to_logical_scenario import Func2LogiConvertor

PARENT = Path.cwd().parent 

def openai_eval(doc, total_user_req, start_number):
    counter, over_all_time = 0, 0.0
    func2logiobject = Func2LogiConvertor()

    ## OPENAI
    ## New prompt without COT generated by gpt evaluation
    prompt_filepath = PARENT / Path("adapt_and_manipulate_scenario/functional_to_json_llms/prompts/open_ai_prompt_without_CoT.txt").resolve()
    eval_outputpath = PARENT / Path("evaluation/func_2_json/open_ai_eval_wo_cot.txt").resolve()

    ## New prompt generated by gpt evaluation
    # prompt_filepath = PARENT / Path("adapt_and_manipulate_scenario/functional_to_json_llms/prompts/open_ai_prompt.txt").resolve()
    # eval_outputpath = PARENT / Path("evaluation/func_2_json/open_ai_eval_out.txt").resolve()

    ## Old prompt generated by john evaluation
    # prompt_filepath = PARENT / Path("adapt_and_manipulate_scenario/functional_to_json_llms/prompts/open_ai_prompt_old.txt").resolve()
    # eval_outputpath = PARENT / Path("evaluation/func_2_json/open_ai_eval_out_old.txt").resolve()

    ## New prompt and GPT3.5
    # prompt_filepath = PARENT / Path("adapt_and_manipulate_scenario/functional_to_json_llms/prompts/open_ai_prompt.txt").resolve()
    # eval_outputpath = PARENT / Path("evaluation/func_2_json/open_ai_gpt3.5_eval_out.txt").resolve()

    # take the prompt from the eval folder
    with open(prompt_filepath, "r") as p:
        prompt = p.read()
    
    for idx in range(start_number, total_user_req+1):
        s = time.time()
        req = doc.split(f"Requirement {idx}:")[1].split(f"Requirement {idx+1}:")[0]
        user_text = req.split('"')[1]
        label = req.split("Label:")[-1]
        true_json_out = json.loads(label.split("\'")[1])
        predicted_json_out = func2logiobject.openai_setup(prompt, user_text)
        print("True Output", true_json_out)
        print("Pred output", predicted_json_out) 

        # compare the results. 
        value = check_true_vs_predicted(true_json_out, predicted_json_out)

        e = time.time()
        over_all_time += e-s
        counter+= value
        # save it as a output text file
        with open(eval_outputpath, "a+") as eval_file:
            if value== 1: 
                result = "SUCCESS"
            elif value==0:
                result = "Failed"
            
            output_iteration = f"Iteration - {idx} \nTrue output - {true_json_out} \nPred Output - {predicted_json_out} \nResult - {result} \nTime taken - {e-s}s \ncorrect prediction - {counter}\noverall time taken - {over_all_time} from {start_number} to {idx} iterations\n \n"
            eval_file.write(output_iteration)

    return counter

def wizardlm_eval(doc, total_user_req, start_number):
    counter, over_all_time = 0, 0.0
    func2logiobject = Func2LogiConvertor()

    prompt_path =  PARENT / Path("adapt_and_manipulate_scenario/functional_to_json_llms/prompts/wizardlm_prompt.txt").resolve()
    ## for cpu
    # eval_outputpath = PARENT / Path("evaluation/func_2_json/wizardlm_eval_out_cpu.txt").resolve()

    ## for gpu
    eval_outputpath = PARENT / Path("evaluation/func_2_json/wizardlm_eval_out_gpu.txt").resolve()

    # print(str(prompt_path))

    # take the prompt from the functional_to_json_llms folder
    with open(prompt_path, "r") as p:
        prompt = p.read()
    
    for idx in range(start_number, total_user_req+1):
        s = time.time()
        req = doc.split(f"Requirement {idx}:")[1].split(f"Requirement {idx+1}:")[0]
        user_text = req.split('"')[1]
        print(user_text)
        label = req.split("Label:")[-1]
        true_json_out = json.loads(label.split("\'")[1])
        predicted_json_out = func2logiobject.wizardlm_opensource_setup(prompt, user_text)
        print("True Output", true_json_out)
        print("Pred output", predicted_json_out) 

        # compare the results. 
        value = check_true_vs_predicted(true_json_out, predicted_json_out)

        e = time.time()
        over_all_time += e-s
        counter+= value
        # save it as a output text file
        with open(eval_outputpath, "a+") as eval_file:
            if value== 1: 
                result = "SUCCESS"
            elif value==0:
                result = "Failed"
            
            output_iteration = f"Iteration - {idx} \nTrue output - {true_json_out} \nPred Output - {predicted_json_out} \nResult - {result} \nTime taken - {e-s}s \ncorrect prediction - {counter}\noverall time taken - {over_all_time} from {start_number} to {idx} iterations\n \n"
            eval_file.write(output_iteration)
    
        print("finished -", idx)
    
    return counter
            
def mistral7b_eval(doc, total_user_req, start_number):
    counter, over_all_time = 0, 0.0
    func2logiobject = Func2LogiConvertor()

    prompt_path =  PARENT / Path("adapt_and_manipulate_scenario/functional_to_json_llms/prompts/mistral_prompt.txt").resolve()
    ## for cpu
    # eval_outputpath = PARENT / Path("evaluation/func_2_json/mistral7b_eval_out.txt").resolve()

    ## for gpu
    eval_outputpath = PARENT / Path("evaluation/func_2_json/mistral7b_eval_out_gpu.txt").resolve()

    # take the prompt from the functional_to_json_llms folder
    with open(prompt_path, "r") as p:
        prompt = p.read()
    
    for idx in range(start_number, total_user_req+1):
        s = time.time()
        req = doc.split(f"Requirement {idx}:")[1].split(f"Requirement {idx+1}:")[0]
        user_text = req.split('"')[1]
        label = req.split("Label:")[-1]
        true_json_out = json.loads(label.split("\'")[1])
        predicted_json_out = func2logiobject.mistral_opensource_setup(prompt, user_text)
        print("True Output", true_json_out)
        print("Pred output", predicted_json_out) 

        # compare the results. 
        value = check_true_vs_predicted(true_json_out, predicted_json_out)

        e = time.time()
        over_all_time += e-s
        counter+= value
        # save it as a output text file
        with open(eval_outputpath, "a+") as eval_file:
            if value== 1: 
                result = "SUCCESS"
            elif value==0:
                result = "Failed"
            
            output_iteration = f"Iteration - {idx} \nTrue output - {true_json_out} \nPred Output - {predicted_json_out} \nResult - {result} \nTime taken - {e-s}s \ncorrect prediction - {counter}\noverall time taken - {over_all_time} from {start_number} to {idx} iterations\n \n"
            eval_file.write(output_iteration)
    
    
    return counter

def orca_2_eval(doc, total_user_req, start_number):
    counter, over_all_time = 0, 0.0
    func2logiobject = Func2LogiConvertor()

    prompt_path =  PARENT / Path("adapt_and_manipulate_scenario/functional_to_json_llms/prompts/orca2_prompt.txt").resolve()
    ## for cpu
    # eval_outputpath = PARENT / Path("evaluation/func_2_json/orca2_eval_out.txt").resolve()

    ## for gpu
    eval_outputpath = PARENT / Path("evaluation/func_2_json/orca2_eval_out_gpu.txt").resolve()

    # take the prompt from the functional_to_json_llms folder
    with open(prompt_path, "r") as p:
        prompt = p.read()
    
    for idx in range(start_number, total_user_req+1):
        s = time.time()
        req = doc.split(f"Requirement {idx}:")[1].split(f"Requirement {idx+1}:")[0]
        user_text = req.split('"')[1]
        label = req.split("Label:")[-1]
        true_json_out = json.loads(label.split("\'")[1])
        predicted_json_out = func2logiobject.orca2_opensource_setup(prompt, user_text)
        print("True Output", true_json_out)
        print("Pred output", predicted_json_out) 

        # compare the results. 
        value = check_true_vs_predicted(true_json_out, predicted_json_out)

        e = time.time()
        over_all_time += e-s
        counter+= value
        # save it as a output text file
        with open(eval_outputpath, "a+") as eval_file:
            if value== 1: 
                result = "SUCCESS"
            elif value==0:
                result = "Failed"
            
            output_iteration = f"Iteration - {idx} \nTrue output - {true_json_out} \nPred Output - {predicted_json_out} \nResult - {result} \nTime taken - {e-s}s \ncorrect prediction - {counter}\noverall time taken - {over_all_time} from {start_number} to {idx} iterations\n \n"
            eval_file.write(output_iteration)

    return counter

def llama_3_eval(doc, total_user_req, start_number):
    counter, over_all_time = 0, 0.0
    func2logiobject = Func2LogiConvertor()

    prompt_path =  PARENT / Path("adapt_and_manipulate_scenario/functional_to_json_llms/prompts/llama3_prompt.txt").resolve()
    ## for cpu
    eval_outputpath = PARENT / Path("evaluation/func_2_json/llama3_eval_out.txt").resolve()

    ## for gpu
    # eval_outputpath = PARENT / Path("evaluation/func_2_json/llama3_eval_out_gpu.txt").resolve()

    # take the prompt from the functional_to_json_llms folder
    with open(prompt_path, "r") as p:
        prompt = p.read()
    
    for idx in range(start_number, total_user_req+1):
        s = time.time()
        req = doc.split(f"Requirement {idx}:")[1].split(f"Requirement {idx+1}:")[0]
        user_text = req.split('"')[1]
        label = req.split("Label:")[-1]
        true_json_out = json.loads(label.split("\'")[1])
        predicted_json_out = func2logiobject.llama3_opensource_setup(prompt, user_text)
        print("True Output", true_json_out)
        print("Pred output", predicted_json_out) 

        # compare the results. 
        value = check_true_vs_predicted(true_json_out, predicted_json_out)

        e = time.time()
        over_all_time += e-s
        counter+= value
        # save it as a output text file
        with open(eval_outputpath, "a+") as eval_file:
            if value== 1: 
                result = "SUCCESS"
            elif value==0:
                result = "Failed"
            
            output_iteration = f"Iteration - {idx} \nTrue output - {true_json_out} \nPred Output - {predicted_json_out} \nResult - {result} \nTime taken - {e-s}s \ncorrect prediction - {counter}\noverall time taken - {over_all_time} from {start_number} to {idx} iterations\n \n"
            eval_file.write(output_iteration)

    return counter

def check_true_vs_predicted(true_json, predicted_json):
    # check the true and predicted are the same
    for true_key, true_value in true_json.items():
        if predicted_json[true_key].lower() == true_value.lower():
            continue
        else:
            return 0
    return 1

    
def eval(model_name, total_user_req, start_number):
    user_req_path =  PARENT / Path("evaluation/functional_scenario_for_eval.txt").resolve()
    ## User Requirements: 
    with open(user_req_path, "r") as file:
        doc = file.read()
    
    # OPENAI EVAL
    if model_name == "openai":
        start_openai = time.time()
        openai_correct_count = openai_eval(doc, total_user_req, start_number)
        end_openai = time.time()
        
        print(f"OpenAI Correct prediction - {openai_correct_count} out of {total_user_req}")
        print(f"OpenAI model took {end_openai-start_openai} seconds for {total_user_req} evaluation")

    elif model_name == "wizardlm":
        start_wizardlm = time.time()
        wizardlm_correct_count = wizardlm_eval(doc, total_user_req, start_number)
        end_wizardlm = time.time()
        
        print(f"Wizardlm Correct prediction - {wizardlm_correct_count} out of {total_user_req}")
        print(f"Wizardlm model took {end_wizardlm-start_wizardlm} seconds for {total_user_req} evaluation")

    elif model_name == "mistral7b":
        start_mistral7b = time.time()
        mistral7b_correct_count = mistral7b_eval(doc, total_user_req, start_number)
        end_mistral7b = time.time()
        
        print(f"Mistral7B Correct prediction - {mistral7b_correct_count} out of {total_user_req}")
        print(f"Mistral7B model took {end_mistral7b-start_mistral7b} seconds for {total_user_req} evaluation")

    elif model_name == "orca2":
        start_orca2 = time.time()
        orca2_correct_count = orca_2_eval(doc, total_user_req, start_number)
        end_orca2 = time.time()
        
        print(f"Orca2 model Correct prediction - {orca2_correct_count} out of {total_user_req}")
        print(f"Orca2 model took {end_orca2-start_orca2} seconds for {total_user_req} evaluation")
    
    elif model_name == "llama3":
        start_llama3 = time.time()
        llama3_correct_count = llama_3_eval(doc, total_user_req, start_number)
        end_llama3 = time.time()
        
        print(f"Llama-3 model Correct prediction - {llama3_correct_count} out of {total_user_req}")
        print(f"Llama-3 model took {end_llama3-start_llama3} seconds for {total_user_req} evaluation")

    else:
        raise Exception("wrong model name provided")
    

if __name__ == "__main__":
    model_name = "openai" # openai, wizardlm, mistral7b, orca2, llama3
    start_number = 1 # starting is from 1 not 0.
    total_user_req = 100
    eval(model_name, total_user_req, start_number)